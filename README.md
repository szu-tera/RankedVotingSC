<div align="center">
  
# Ranked Voting based Self-Consistency of Large Language Models

[![Paper](https://img.shields.io/badge/arxiv-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)]([https://arxiv.org/abs/2504.16084](https://arxiv.org/abs/2505.10772))  [![Github](https://img.shields.io/badge/GitHub-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white)]([https://github.com/PRIME-RL/TTRL](https://github.com/szu-tera/RankedVotingSC))

<div align="center" style="font-family: Arial, sans-serif;">
  <p>
    <a href="#news" style="text-decoration: none; font-weight: bold;">üéâ News</a> ‚Ä¢
    <a href="#overview" style="text-decoration: none; font-weight: bold;">üìå Overview</a> ‚Ä¢
    <a href="#project-structure" style="text-decoration: none; font-weight: bold;">üìÅ Project Structure</a>
  </p>
  <p>
    <a href="#getting-started" style="text-decoration: none; font-weight: bold;">‚ú® Getting Started</a> ‚Ä¢
    <a href="#evaluation" style="text-decoration: none; font-weight: bold;">üìÉ Evaluation</a> ‚Ä¢
    <a href="#contact" style="text-decoration: none; font-weight: bold;">üì® Contact</a> ‚Ä¢
    <a href="#citation" style="text-decoration: none; font-weight: bold;">üéà Citation</a>
  </p>
</div>

</div>




## üéâ News

- **[2025/05/16]** We Released our Paper on arXiv.

- **[2025/05/15]** Our paper is accepted by ACL 2025 as Findings. 

- **[2025/04/28]** We release our code for RankedVotingSC.

## üìåOverview

Ranked Voting based Self-Consistency (RankedVotingSC) is a method that improves large language models‚Äô reasoning by generating ranked answers and applying ranked voting across multiple outputs. Experiments on multiple-choice and open-ended QA tasks show that RankedVotingSC outperforms traditional majority voting by producing more reliable final answers.

<p align="center">
   <img src="figs/Main.jpg" alt="" style="width: 90%;">
</p>


## üìÅProject Structure

The repository is organized as follows:

```
RankedVotingSC/
‚îú‚îÄ‚îÄ RankedVotingSC/             # core source package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ RankBasedSC.py
‚îú‚îÄ‚îÄ data/                       # data generated by different methods
‚îú‚îÄ‚îÄ figs/                       # figures used in the paper
‚îú‚îÄ‚îÄ lm-evaluation-harness/      # generation + evaluation framework dependency
‚îú‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ evaluate.py                 # evaluation script
‚îú‚îÄ‚îÄ evaluate.sh                 # command-line runner
‚îú‚îÄ‚îÄ requirements.txt            # required packages
‚îî‚îÄ‚îÄ utils.py                    # utility helpers
```

## ‚ú®Getting Started

Clone our repository and install the required environment:

```shell
# Clone the repository
git clone https://github.com/szu-tera/RankedVotingSC.git

# install required packages
cd RankedVotingSC
pip install -r requirements.txt

# Install lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

## üìÉEvaluation

First, define a task YAML file under `code/lm-evaluation-harness/lm_eval/tasks/`.  
For example, the task configuration for **Gemma-2-9b-it** on **CommonsenseQA (CSQA)** can be found [here](https://github.com/szu-tera/RankedVotingSC/blob/main/lm-evaluation-harness/lm_eval/tasks/CSQA/CSQA.yaml) (If you need to implement a new task, please refer to [this guide](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md)).

Next, we provide a step-by-step demonstration using **Gemma-2-9b-it** on the **CSQA** dataset (example outputs [here](https://github.com/szu-tera/RankedVotingSC/blob/main/data/samples.json)) as an example:

---

### Step 1: Generate Model Outputs

Run the following command to generate outputs from the model:

```shell
accelerate launch -m lm_eval \
    --model hf \
    --model_args pretrained=[path/to/your/model],dtype=bfloat16 \
    --apply_chat_template \
    --fewshot_as_multiturn \
    --log_samples \
    --output_path RankedVotingSC/data \
    --tasks CSQA-RankedVoting \
    --batch_size auto
```

> **Note:** Replace `[path/to/your/model]` with your local model path or the Huggingface model ID.  

---

Arguments for the generation script are as follows,

- `--model`: Specify the format of the model to use.
  - `hf`: Use Huggingface format.

- `--model_args`: Specify the model path and the precision type.
  - `pretrained`: Path to the model on Huggingface or local directory.
  - `dtype`: Precision type for loading (e.g., bfloat16).

- `--apply_chat_template`: Apply the chat template formatting to input prompts.

- `--fewshot_as_multiturn`: Format few-shot examples as multi-turn dialogues.

- `--log_samples`: Save generated samples for further filtering and voting.

- `--output_path`: Specify the directory where output files (e.g., `RankedVotingSC/data`) will be saved.

- `--tasks`: Specify the evaluation task (e.g., `CSQA-RankedVoting`).

- `--batch_size`: Specify the batch size for generation.
  - `auto`: Automatically adjust the batch size based on available GPU memory.

---

### Step 2: Perform Ranked Voting and Evaluation

After generating the outputs, use the following command to conduct Ranked Voting and evaluate performance:

```shell
python evaluate.py \
    --cot_file data/cot_samples.json \
    --bon_file data/bon_samples.json \
    --sc_file data/sc_samples.json \
    --ac_file data/ac_samples.json \
    --rc_file data/rc_samples.json \
```

You can also use `evaluate.sh` to instead.

---

Arguments for the evaluation script are as follows,

- `--cot_file`: Path to CoT (Chain of Thought) generation results (optional).

- `--bon_file`: Path to BoN (Best of N) generation results (optional).

- `--sc_file`: Path to SC (Self-Consistency) generation results (optional).

- `--ac_file`: Path to AC (Adaptive Consistency) generation results (optional).

- `--rc_file`: Path to the Ranked Consistency input file (required).

### Expected Output

```
+---------+----------+----------+----------+----------+----------+----------+----------+
| Methods |   CoT    |   BoN    |    SC    |    AC    |   IRV    |   BCV    |   MRRV   |
+---------+----------+----------+----------+----------+----------+----------+----------+
|   Accs  | 0.799345 | 0.806716 | 0.804259 | 0.812449 | 0.832105 | 0.829648 | 0.830467 |
+---------+----------+----------+----------+----------+----------+----------+----------+
```

## üì®Contact

- Weiqin Wang: here1swqw@gmail.com

## üéàCitation

If you find this repository useful for your research, please consider citing our paper:

```bibtex
@misc{wang2025rankedvotingbasedselfconsistency,
      title={Ranked Voting based Self-Consistency of Large Language Models}, 
      author={Weiqin Wang and Yile Wang and Hui Huang},
      year={2025},
      eprint={2505.10772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.10772}, 
}
```

  
